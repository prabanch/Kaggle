{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle link - https://www.kaggle.com/code/prabanch/jigsaw-unintended-bias-in-toxicity-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded\n"
     ]
    }
   ],
   "source": [
    "# Scikit Learn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score,roc_auc_score,roc_curve\n",
    "\n",
    "from sklearn import model_selection, preprocessing, metrics, ensemble\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\n",
    "from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D, Flatten, MaxPooling1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "import gc\n",
    "\n",
    "print(\"Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/thousandvoices/simple-lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train.csv', 'test.csv', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('../input/jigsaw-unintended-bias-in-toxicity-classification'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a06d6485c4fb4b16cde71699486723c3f0035241"
   },
   "source": [
    "**Read the data******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "719db1055eea23f98cbda39c6bf227bd7e6b0645"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\n",
    "\n",
    "# train_df =   train_df.head(1000)\n",
    "# test_df =   test_df.head(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_1 = train[train['target']==1]\n",
    "# Train_0 = train[train['target']==0]\n",
    "# train =  pd.concat([Train_1,Train_0.head(len(Train_1))], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[train['target']==1].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "50d7f1c0fdf4aea206afdacb9db5007a2c62c2ee"
   },
   "source": [
    "**Check the target column break down**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "2afd3095b6715f7704a4848a9d067a3bc129316a"
   },
   "outputs": [],
   "source": [
    "# train['target'] = np.where(train['target'] >= 0.5, 1, 0)\n",
    "# train[\"target\"].value_counts()\n",
    "\n",
    "NUM_MODELS = 2\n",
    "BATCH_SIZE = 512\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "EPOCHS = 4\n",
    "MAX_LEN = 220\n",
    "IDENTITY_COLUMNS = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n",
    "]\n",
    "AUX_COLUMNS = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n",
    "TEXT_COLUMN = 'comment_text'\n",
    "TARGET_COLUMN = 'target'\n",
    "CHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n",
    "\n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n",
    "\n",
    "\n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return embedding_matrix\n",
    "\n",
    "for column in IDENTITY_COLUMNS + [TARGET_COLUMN]:\n",
    "    train_df[column] = np.where(train_df[column] >= 0.5, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4bcd51b27e18083305894ab7ad4d0512ff6acc6d"
   },
   "source": [
    "**Word Cloud**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f0d7f44deca7e7b8b962238a59f0df4cbb9e00d9"
   },
   "source": [
    "* **1) Word cloud of Non toxic comments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "f3b0eeaf667f088235e9f618f7b4a75c588e24f2"
   },
   "outputs": [],
   "source": [
    "# stopwords = set(STOPWORDS)\n",
    "\n",
    "\n",
    "# wordcloud = WordCloud(\n",
    "#                           background_color='white',\n",
    "#                           stopwords=stopwords,\n",
    "#                           max_words=200,\n",
    "#                           max_font_size=40, \n",
    "#                           random_state=42\n",
    "#                          ).generate(str(train[train.target==0]['comment_text']))\n",
    "\n",
    "# print(wordcloud)\n",
    "# fig = plt.figure(1)\n",
    "# plt.imshow(wordcloud)\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "# fig.savefig(\"word1.png\", dpi=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "64ed082363b637ed1d8cce75fb2678dc20776780"
   },
   "source": [
    "* **2) Word cloud of toxic comments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "68a5c228ccc86c6f914f8e7cd4f540c44bd2da4f"
   },
   "outputs": [],
   "source": [
    "# stopwords = set(STOPWORDS)\n",
    "\n",
    "# wordcloud = WordCloud(\n",
    "#                           background_color='white',\n",
    "#                           stopwords=stopwords,\n",
    "#                           max_words=200,\n",
    "#                           max_font_size=40, \n",
    "#                           random_state=42\n",
    "#                          ).generate(str(train[train.target==1]['comment_text']))\n",
    "\n",
    "# print(wordcloud)\n",
    "# fig = plt.figure(1)\n",
    "# plt.imshow(wordcloud)\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "# fig.savefig(\"word1.png\", dpi=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "a6d9634e5004e9ffd5854938d3b9ed631b20b831"
   },
   "outputs": [],
   "source": [
    "## split to train and val\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=2018)\n",
    "\n",
    "## some config values \n",
    "embed_size = 50 # how big is each word vector\n",
    "max_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 50 # max number of words in a question to use\n",
    "\n",
    "## fill up the missing values\n",
    "train_X = train_df[\"comment_text\"].fillna(\"_na_\").values\n",
    "val_X = val_df[\"comment_text\"].fillna(\"_na_\").values\n",
    "test_X = test_df[\"comment_text\"].fillna(\"_na_\").values\n",
    "\n",
    "## Tokenize the sentences\n",
    "tokenizer = Tokenizer(filters=CHARS_TO_REMOVE)\n",
    "tokenizer.fit_on_texts(list(train_X)+list(val_X)+list(test_X))\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "val_X = tokenizer.texts_to_sequences(val_X)\n",
    "test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "## Pad the sentences \n",
    "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "val_X = pad_sequences(val_X, maxlen=maxlen)\n",
    "test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "\n",
    "## Get the target values\n",
    "train_y = train_df['target'].values\n",
    "val_y = val_df['target'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "717fb2b8fc1283bb8c2db5f90a8125a35694ebfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            2500000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 43, 32)            12832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 672)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               344576    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 3,021,761\n",
      "Trainable params: 3,021,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1624386 samples, validate on 180488 samples\n",
      "Epoch 1/1\n",
      "1624386/1624386 [==============================] - 24s 15us/step - loss: 0.2282 - acc: 0.9257 - val_loss: 0.1925 - val_acc: 0.9346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f705e129fd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# inp = Input(shape=(maxlen,))\n",
    "# x = Embedding(max_features, embed_size)(inp)\n",
    "# x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "# x = GlobalMaxPool1D()(x)\n",
    "# x = Dense(16, activation=\"relu\")(x)\n",
    "# x = Dropout(0.1)(x)\n",
    "# x = Dense(1, activation=\"sigmoid\")(x)\n",
    "# model = Model(inputs=inp, outputs=x)\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# print(model.summary())\n",
    "# model.fit(train_X, train_y, batch_size=512, epochs=2, validation_data=(val_X, val_y))\n",
    "\n",
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_size, input_length=maxlen))\n",
    "\n",
    "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "model.fit(train_X, train_y, batch_size=512, epochs=1, validation_data=(val_X, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score,roc_auc_score,roc_curve, auc,  f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "# Making the Confusion Matrix\n",
    "def get_metrics(y_test, y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    class_names=[0,1] # name  of classes\n",
    "    fig, ax = plt.subplots()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    # create heatmap\n",
    "    sns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "    ax.xaxis.set_label_position(\"top\")\n",
    "    plt.tight_layout()\n",
    "    plt.title('Confusion matrix', y=1.1)\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "    print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    # Model Precision: what percentage of positive tuples are labeled as such?\n",
    "    print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "\n",
    "    # Model Recall: what percentage of positive tuples are labelled as such?\n",
    "    print(\" True positive rate or (Recall or Sensitivity) :\",metrics.recall_score(y_test, y_pred))\n",
    "\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_test, y_pred).ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "\n",
    "    #Specitivity. or True negative rate\n",
    "    print(\" True Negative rate or Specitivity :\",specificity)\n",
    "\n",
    "    false_negative = fn / (fn+tp)\n",
    "\n",
    "    #False negative rate\n",
    "    print(\" False Negative rate :\",false_negative)\n",
    "\n",
    "    #False positive rate\n",
    "    print(\" False positive rate (Type 1 error) :\",1 - specificity)\n",
    "    \n",
    "    print('F Score', f1_score(y_test, y_pred))\n",
    "    print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_pred_y = model.predict_classes([val_X], batch_size=1024, verbose=1)\n",
    "# get_metrics(val_y,val_pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_text = ['../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec']\n",
    "\n",
    "fast_text_matrix = np.concatenate([build_matrix(tokenizer.word_index, f) for f in fast_text], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 50, 300)           98517000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 50, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 50, 256)           440320    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 50, 256)           395264    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 99,615,753\n",
      "Trainable params: 1,098,753\n",
      "Non-trainable params: 98,517,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 1624386 samples, validate on 180488 samples\n",
      "Epoch 1/20\n",
      "1624386/1624386 [==============================] - 146s 90us/step - loss: 0.1548 - acc: 0.9436 - val_loss: 0.1394 - val_acc: 0.9484\n",
      "Epoch 2/20\n",
      "1624386/1624386 [==============================] - 145s 89us/step - loss: 0.1390 - acc: 0.9485 - val_loss: 0.1333 - val_acc: 0.9499\n",
      "Epoch 3/20\n",
      "1624386/1624386 [==============================] - 145s 89us/step - loss: 0.1344 - acc: 0.9500 - val_loss: 0.1314 - val_acc: 0.9513\n",
      "Epoch 4/20\n",
      " 288256/1624386 [====>.........................] - ETA: 1:53 - loss: 0.1301 - acc: 0.9515"
     ]
    }
   ],
   "source": [
    "## some config values \n",
    "embed_size = 50 # how big is each word vector\n",
    "max_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 50 # max number of words in a question to use\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(fast_text_matrix), 300, input_length=maxlen, weights=[fast_text_matrix], trainable=False))\n",
    "\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(Bidirectional(CuDNNLSTM(128, return_sequences=True)))\n",
    "model.add(Bidirectional(CuDNNLSTM(128, return_sequences=True)))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "model.fit(train_X, train_y, batch_size=512, epochs=20, validation_data=(val_X, val_y))\n",
    "\n",
    "get_metrics(val_y,model.predict_classes([val_X], batch_size=1024, verbose=1))\n",
    "\n",
    "fast_text_val_pred_y = model.predict([test_X], batch_size=1024, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3151"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del fast_text_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "glove = ['../input/glove840b300dtxt/glove.840B.300d.txt']\n",
    "glove_matrix = np.concatenate([build_matrix(tokenizer.word_index, f) for f in glove], axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 50, 300)           98517000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 50, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 50, 256)           440320    \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 50, 256)           395264    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 99,615,753\n",
      "Trainable params: 1,098,753\n",
      "Non-trainable params: 98,517,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1624386 samples, validate on 180488 samples\n",
      "Epoch 1/20\n",
      "1624386/1624386 [==============================] - 146s 90us/step - loss: 0.1531 - acc: 0.9442 - val_loss: 0.1380 - val_acc: 0.9489\n",
      "Epoch 2/20\n",
      "1624386/1624386 [==============================] - 145s 89us/step - loss: 0.1382 - acc: 0.9488 - val_loss: 0.1337 - val_acc: 0.9506\n",
      "Epoch 3/20\n",
      "1624386/1624386 [==============================] - 145s 89us/step - loss: 0.1334 - acc: 0.9503 - val_loss: 0.1317 - val_acc: 0.9508\n",
      "Epoch 4/20\n",
      "1624386/1624386 [==============================] - 145s 90us/step - loss: 0.1294 - acc: 0.9516 - val_loss: 0.1315 - val_acc: 0.9508\n",
      "Epoch 5/20\n",
      "1624386/1624386 [==============================] - 145s 89us/step - loss: 0.1260 - acc: 0.9527 - val_loss: 0.1345 - val_acc: 0.9499\n",
      "Epoch 6/20\n",
      "1624386/1624386 [==============================] - 145s 89us/step - loss: 0.1225 - acc: 0.9537 - val_loss: 0.1326 - val_acc: 0.9508\n",
      "Epoch 7/20\n",
      "1624386/1624386 [==============================] - 146s 90us/step - loss: 0.1216 - acc: 0.9548 - val_loss: 0.1339 - val_acc: 0.9504\n",
      "Epoch 8/20\n",
      "1406976/1624386 [========================>.....] - ETA: 18s - loss: 0.1158 - acc: 0.9558"
     ]
    }
   ],
   "source": [
    "## some config values \n",
    "embed_size = 50 # how big is each word vector\n",
    "max_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 50 # max number of words in a question to use\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(glove_matrix), 300, input_length=maxlen, weights=[glove_matrix], trainable=False))\n",
    "\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(Bidirectional(CuDNNLSTM(128, return_sequences=True)))\n",
    "model.add(Bidirectional(CuDNNLSTM(128, return_sequences=True)))\n",
    "\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "model.fit(train_X, train_y, batch_size=512, epochs=20, validation_data=(val_X, val_y))\n",
    "\n",
    "get_metrics(val_y,model.predict_classes([val_X], batch_size=1024, verbose=1))\n",
    "\n",
    "glove_val_pred_y = model.predict([test_X], batch_size=1024, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3169"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del glove_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions =  (fast_text_val_pred_y + glove_val_pred_y) / 2\n",
    "final_predictions = np.where(final_predictions >= 0.5, 1, 0)\n",
    "output = pd.DataFrame({\"id\":test_df[\"id\"].values})\n",
    "output['prediction'] = final_predictions\n",
    "output.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
